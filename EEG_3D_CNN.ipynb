{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This file is used for classification of emotion-based EEG signals from PD and HC using 3D Convolutional Neural Network (3D-CNN). The input to 3D-CNN, *EEG Movie*, are obtained using `eeg_to_images.ipynb` file. The detailed architecture, input and output dimensions are explained in the paper. The highlighted part in the following image indicate the pipeline of the current file.\n",
    "\n",
    "Note: The configurations, like hyperparameter values, present in this file are not the final configurations as reported in the paper. The present configurations are maybe mutated for additional experiments. However, results reported in the paper can be reproduced by plugging in the appropriate configurations as mentioned in the paper.\n",
    "\n",
    "![3D CNN pipeline associated with this notebook](images/3dcnn.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from keras.layers import Conv3D, AveragePooling3D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks  import EarlyStopping\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score,roc_auc_score, roc_curve, auc\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense, Flatten, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.callbacks  import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Path to the pre-processed features\n",
    "dataPath = '../data/spectral_feat_tensor_full_with_full_labels_images_3dcnn_norm.mat'\n",
    "\n",
    "# Path to save features, results, etc.\n",
    "savePath = '../results/CNN_3D_our/'\n",
    "\n",
    "# Experiment name\n",
    "experiment = 'MultiClass_PD'\n",
    "\n",
    "filename = savePath+'CNN_3D_results_'+experiment+'.mat'\n",
    "plot_title = 'Categorical Emotion Classification - PD'\n",
    "\n",
    "# --- Model parameters ---\n",
    "\n",
    "nb_filters = [16, 32, 32, 64, 128]  # Number of filters for convolution\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "stride_size = 2\n",
    "padding = 'same'\n",
    "weight_decay = 0.000001\n",
    "dense_layer_neuron_num = 128\n",
    "epochs = 50\n",
    "momentum = 0.8\n",
    "\n",
    "# Load data\n",
    "matContent = sio.loadmat(dataPath)\n",
    "features = matContent['pd_feat_img']\n",
    "print(\"features shape:\", features.shape)\n",
    "labels = np.squeeze(matContent['pd_multi_labels'])\n",
    "labels[labels < 0] = 0\n",
    "features = np.swapaxes(features,2,4)\n",
    "features = np.swapaxes(features,2,3)\n",
    "# Labels as 0, 1, .., 5, rather than 1, 2, .., 6\n",
    "labels[labels == 6] = 0\n",
    "#labels = labels.astype(int)\n",
    "\n",
    "#df = pd.read_csv(dataPath, header = None)\n",
    "#features = df.iloc[1:,:-2].to_numpy()\n",
    "#labels = df.iloc[1:,-1].to_numpy() # last but one column for PD vs NC\n",
    "\n",
    "#dict_hvlv = {1:0, 2:1, 3:0, 4:0, 5:1, 6:0} #HVLV labels mapping dictionary\n",
    "#labels = labels.map(dict_hvlv).to_numpy()\n",
    "#labels[labels == 1] = 0\n",
    "#labels[labels == 2] = 1\n",
    "#labels[labels == 3] = 1\n",
    "#labels[labels == 4] = 1\n",
    "#labels[labels == 5] = 1\n",
    "#labels[labels == 6] = 1\n",
    "#labels[labels < 0]=0\n",
    "#labels[labels == 6]=0\n",
    "\n",
    "# randomise the sample sequence\n",
    "rand_order = np.arange(features.shape[0])\n",
    "np.random.shuffle(rand_order)\n",
    "features = features[rand_order,]\n",
    "labels = np.squeeze(labels[rand_order,])\n",
    "class_num = np.size(np.unique(labels))\n",
    "labels_categorical = np_utils.to_categorical(labels, class_num)\n",
    "del matContent\n",
    "\n",
    "print('Features shape:', features.shape)\n",
    "print('Number of classes:', class_num)\n",
    "print('Unique labels:', np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create, and compile Keras model\n",
    "def create_model(init_mode, activation, dropout_rate, optimizer, learn_rate):\n",
    "  model = Sequential()\n",
    "  model.add(Conv3D(filters=nb_filters[0], kernel_size=kernel_size, padding=padding,\n",
    "                   activation=activation, input_shape=(features.shape[1], features.shape[2],\n",
    "                                                       features.shape[3], features.shape[4]), trainable=True))\n",
    "  model.add(AveragePooling3D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  model.add(Conv3D(filters=nb_filters[1], kernel_size=kernel_size, padding=padding,\n",
    "                   activation=activation, kernel_initializer=init_mode, trainable=True))\n",
    "  model.add(AveragePooling3D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  model.add(Conv3D(filters=nb_filters[2], kernel_size=kernel_size, padding=padding,\n",
    "                   activation=activation, kernel_initializer=init_mode, trainable=True))\n",
    "  model.add(AveragePooling3D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  # ####added by me#####\n",
    "  #model.add(Conv1D(filters=nb_filters[3], kernel_size=kernel_size, padding=padding, activation=activation,\n",
    "  #              kernel_initializer='he_normal'))\n",
    "  #model.add(AveragePooling1D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  #model.add(Conv1D(filters=nb_filters[4], kernel_size=kernel_size, padding=padding, activation=activation,\n",
    "  #              kernel_initializer='he_normal'))\n",
    "  #model.add(AveragePooling1D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  # ####added by me#####\n",
    "  model.add(Flatten())\n",
    "  model.add(BatchNormalization(epsilon=0.001))\n",
    "  model.add(Dense(dense_layer_neuron_num, kernel_initializer=init_mode, activation=activation))\n",
    "  model.add(Dropout(dropout_rate))\n",
    "  model.add(Dense(class_num))\n",
    "  model.add(Activation('softmax'))\n",
    "  #model.summary()\n",
    "  #model.load_weights('Gender_notClean_HIweights.hdf5')\n",
    "  #earlyStopping = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
    "  if optimizer == 'SGD':\n",
    "    opt = SGD(learning_rate=learn_rate / 10 ** epochs, momentum = momentum, decay = weight_decay, nesterov = True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "  elif optimizer == 'Adam':\n",
    "    opt = Adam()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "  elif optimizer == 'RMSprop':\n",
    "    opt = RMSprop(learning_rate=learn_rate, epsilon=1e-07)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Build Classifier for Grid Search\n",
    "cnn_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "#batch_size = [16,32]\n",
    "#epochs = [5,10,15]\n",
    "#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'he_normal', 'he_uniform']\n",
    "#activation = ['softmax', 'softsign', 'relu', 'tanh', 'sigmoid', 'linear']\n",
    "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "#neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "learn_rate = [0.001, 0.01, 0.1]\n",
    "optimizer = ['SGD', 'Adam','RMSprop']\n",
    "#momentum = [0.8,0.9]\n",
    "init_mode = ['he_normal','he_uniform']\n",
    "activation = ['relu','tanh']\n",
    "dropout_rate = [0.3,0.4,0.5]\n",
    "foldNum = 10\n",
    "\n",
    "# Parameter space for Grid Search\n",
    "p_grid = dict(init_mode=init_mode, dropout_rate=dropout_rate, activation=activation,\n",
    "              optimizer=optimizer, learn_rate=learn_rate)\n",
    "              #, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=cnn_model, param_grid=p_grid,\n",
    "                    cv=foldNum, verbose=0)\n",
    "# Standerdize\n",
    "#feat_shape = features.shape\n",
    "#features = np.reshape(features, (feat_shape[0], feat_shape[1]*feat_shape[2]))\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(features)\n",
    "#scaleFeatures = scaler.transform(features)\n",
    "#scaleFeatures = np.reshape(scaleFeatures, (features.shape[0], 6, -1))\n",
    "\n",
    "print('Performing Gridsearch')\n",
    "grid_result = grid.fit(features,labels_categorical)\n",
    "best_params = grid_result.best_params_\n",
    "print('Best parameters:', best_params)\n",
    "tf.keras.backend.clear_session()\n",
    "estimator = create_model(init_mode=best_params.get('init_mode'), learn_rate=best_params.get('learn_rate'), \n",
    "                         optimizer=best_params.get('optimizer'), activation=best_params.get('activation'), \n",
    "                         dropout_rate=best_params.get('dropout_rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(array(['tanh'], dtype='<U4'), array([[0.3]]), array(['he_uniform'], dtype='<U10'), array([[0.01]]), array(['Adam'], dtype='<U4'))]]\n"
     ]
    }
   ],
   "source": [
    "mat_path = '../results/CNN_3D_our/CNN_3D_results_MultiClass_PD.mat'\n",
    "params = sio.loadmat(mat_path)\n",
    "best_params = params['best_params']\n",
    "del mat_path\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "estimator = create_model(init_mode='he_normal', learn_rate=0.01, optimizer='Adam', activation='tanh', \n",
    "                         dropout_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#conf_mat = np.zeros((2,2))\n",
    "foldNum = 10\n",
    "conf_mat = np.zeros((6,6))\n",
    "f = 0\n",
    "cm_list = []\n",
    "val_loss = []\n",
    "val_accuracy = []\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "f1_MacroNet = np.zeros([foldNum,]) \n",
    "f1_weightedNet = np.zeros([foldNum,])\n",
    "precisionNet = np.zeros([foldNum,])\n",
    "recallNet = np.zeros([foldNum,])\n",
    "accNet = np.zeros([foldNum,])\n",
    "\n",
    "#channels = features.shape[2] # number of channels 14\n",
    "\n",
    "# Initialise Stratified K Fold\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=100, shuffle=True)\n",
    "\n",
    "for train, test in kfold.split(features, labels):\n",
    "    trainingFeatures = features[train,:,:, :]\n",
    "    testFeatures = features[test,:,:, :]\n",
    "    train_shape = trainingFeatures.shape\n",
    "    test_shape = testFeatures.shape\n",
    "    tf.keras.backend.clear_session()#to clear any previosuly obtiained models\n",
    "    estimator = create_model(init_mode=best_params.get('init_mode'), learn_rate=best_params.get('learn_rate'),\n",
    "                         optimizer=best_params.get('optimizer'), activation=best_params.get('activation'),\n",
    "                         dropout_rate=best_params.get('dropout_rate'))\n",
    "    estimator.summary()\n",
    "    dummy = estimator.fit(trainingFeatures, labels_categorical[train,:], batch_size=32, \n",
    "                  epochs=epochs, verbose=2, validation_split=0.1)\n",
    "    \n",
    "    predicted_labelsNet = estimator.predict_classes(testFeatures, verbose=0)\n",
    "    predicted_probsNet = estimator.predict_proba(testFeatures,batch_size=1,verbose=0)\n",
    "    #cm = confusion_matrix(labels[test,], predicted_labelsNet, labels=[0,1])\n",
    "    cm = confusion_matrix(labels[test,], predicted_labelsNet, labels=[1,2,3,4,5,0])\n",
    "    cm = cm/cm.sum(axis=1, keepdims=True)\n",
    "    #conf_mat = conf_mat+cm\n",
    "    cm_list.append(cm)\n",
    "\n",
    "    precisionNet[f] = precision_score(labels[test,], predicted_labelsNet, average='macro')\n",
    "    recallNet[f] = recall_score(labels[test,], predicted_labelsNet, average='macro')\n",
    "    f1_MacroNet[f] = f1_score(labels[test,], predicted_labelsNet, average='macro')\n",
    "    f1_weightedNet[f] = f1_score(labels[test,], predicted_labelsNet, average='weighted')\n",
    "    accNet[f] = accuracy_score(labels[test,], predicted_labelsNet)\n",
    "    print(experiment + '_CNN: Fold %d : f1_macroscore: %.4f' % (f + 1, f1_MacroNet[f]))\n",
    "    print(experiment + '_CNN: Fold %d : f1_weightedscore: %.4f' % (f + 1, f1_weightedNet[f]))\n",
    "    print(experiment + '_CNN: Fold %d : acc: %.4f' % (f + 1, accNet[f]))\n",
    "    f += 1\n",
    "    train_loss.append(dummy.history['loss'])\n",
    "    val_loss.append(dummy.history['val_loss'])\n",
    "    train_accuracy.append(dummy.history['accuracy'])\n",
    "    val_accuracy.append(dummy.history['val_accuracy'])\n",
    "\n",
    "# Dump all the results\n",
    "#xc = range(1,epochs+1)\n",
    "tr_loss_df = pd.DataFrame(train_loss)\n",
    "tr_acc_df = pd.DataFrame(train_accuracy)\n",
    "val_loss_df = pd.DataFrame(val_loss)\n",
    "val_acc_df = pd.DataFrame(val_accuracy)\n",
    "\n",
    "train_loss_mlist = tr_loss_df.mean(axis=0).to_numpy()\n",
    "train_loss_slist = tr_loss_df.std(axis=0).to_numpy()\n",
    "train_acc_mlist = tr_acc_df.mean(axis=0).to_numpy() \n",
    "train_acc_slist = tr_acc_df.std(axis=0).to_numpy() \n",
    "val_loss_mlist = val_loss_df.mean(axis=0).to_numpy() \n",
    "val_loss_slist = val_loss_df.std(axis=0).to_numpy() \n",
    "val_acc_mlist = val_acc_df.mean(axis=0).to_numpy()\n",
    "val_acc_slist = val_acc_df.std(axis=0).to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "y1 = train_loss_mlist\n",
    "e1 = train_loss_slist\n",
    "y2 = val_loss_mlist\n",
    "e2 = val_loss_slist\n",
    "x1 = range(1, len(train_loss_mlist)+1)\n",
    "a1 = plt.plot(x1, y1, color='#089FFF', label='Training Loss')\n",
    "plt.fill_between(x1, y1-e1, y1+e1, alpha=0.3, facecolor='#089FFF', linewidth=4)\n",
    "a2 = plt.plot(x1, y2, color='#cc9106', label='Validation Loss')\n",
    "plt.fill_between(x1, y2-e2, y2+e2, alpha=0.3, facecolor='#cc9106', linewidth=4)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "y3 = train_acc_mlist\n",
    "e3 = train_acc_slist\n",
    "y4 = val_acc_mlist\n",
    "e4 = val_acc_slist\n",
    "x2 = range(1, len(train_acc_mlist)+1)\n",
    "a1 = plt.plot(x2, y3, color='#089FFF', label='Training Accuracy')\n",
    "plt.fill_between(x2, y3-e3, y3+e3, alpha=0.3, facecolor='#089FFF', linewidth=4)\n",
    "a2 = plt.plot(x2, y4, color='#cc9106', label='Validation Accuracy')\n",
    "plt.fill_between(x2, y4-e4, y4+e4, alpha=0.3, facecolor='#cc9106', linewidth=4)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig(filename[:-4]+'_loss_acc_curve'+'.png')\n",
    "plt.show()\n",
    "\n",
    "# Visualise confusion matrix\n",
    "conf_mat = np.mean(cm_list, axis=0)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax = sns.heatmap(conf_mat, cmap='YlGnBu', annot = True, fmt='.4f', vmin=0, vmax=1, annot_kws = {'fontsize':12})\n",
    "#ax.set_yticklabels(['LA', 'HA'], rotation = 0)\n",
    "#ax.set_xticklabels(['LA', 'HA'], rotation = 0)\n",
    "ax.set_yticklabels(['Sad', 'Happy', 'Fear', 'Disgust', 'Surprise', 'Anger'], rotation = 0)\n",
    "ax.set_xticklabels(['Sad', 'Happy', 'Fear', 'Disgust', 'Surprise', 'Anger'], rotation = 0)\n",
    "\n",
    "ax.set_title(plot_title)\n",
    "ax.get_figure().savefig(filename[:-4]+'_conf_mat'+'.png')\n",
    "plt.show()\n",
    "\n",
    "print('Mean and std of F1 MACRO is %.4f +- %.4f' % (np.mean(f1_MacroNet), np.std(f1_MacroNet)))\n",
    "print('Mean and std of F1 WEIGHTED is %.4f +- %.4f' % (np.mean(f1_weightedNet), np.std(f1_weightedNet)))\n",
    "print('Mean and std of accuracy is %.4f +- %.4f' % (np.mean(accNet), np.std(accNet)))\n",
    "\n",
    "# Save results\n",
    "\n",
    "sio.savemat(filename, {'precisionNet': precisionNet,'recallNet': recallNet, 'f1_MacroNet': f1_MacroNet,\n",
    "                       'f1_weightedNet':f1_weightedNet,'accNet':accNet, 'conf_mat':conf_mat, 'conf_mat_list':cm_list,\n",
    "                       'best_params':best_params, 'experiment':experiment,'nb_filters':nb_filters,\n",
    "                       'kernel_size':kernel_size, 'pool_size':pool_size, 'stride_size':stride_size,'padding':padding,\n",
    "                       'weight_decay':weight_decay, 'dense_layer_neuron_num':dense_layer_neuron_num,'epochs':epochs,\n",
    "                      'train_loss':train_loss, 'train_accuracy':train_accuracy, 'val_loss':val_loss,\n",
    "                       'val_accuracy':val_accuracy})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}